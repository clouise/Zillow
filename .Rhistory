gc()
View(prop2)
str(prop2)
mean((prop2$longitude)
)
mean((prop2$longitude), na.rm=TRUE)/1e6
head(prop2$longitude)
View(t)
View(t)
train <- merge(prop, t, by="parcelid", all.y=TRUE)
train <- merge(prop2, t, by="parcelid", all.y=TRUE)
View(train)
rm(t)
rm(prop2)
rm(sample)
p[, logerror := 0]
gc()
set.seed(181)
maeSummary <- function (train,
lev = NULL,
model = NULL) {
out <- mae(train$obs, train$pred)
names(out) <- "MAE"
out
}
control <- trainControl(method = "cv",
number = 5,
verboseIter=TRUE,
summaryFunction = maeSummary
)
grid <- expand.grid(depth = c(5),
learning_rate = c(0.0005),
iterations = c(800),
l2_leaf_reg = c(1e-3),
rsm = c(0.95),
border_count = c(32)
)
ncol(train)
traindata <- train[,2:(ncol(train)-1)]
View(traindata)
target <- train[,logerror]
qplot(train$logerror)
train %>% filter(logerr != 0) %>% qplot(train$logerror)
train %>% filter(logerror != 0) %>% qplot(train$logerror)
maeSummary <- function (train,
lev = NULL,
model = NULL) {
out <- mae(train$obs, train$pred)
names(out) <- "MAE"
out
}
control <- trainControl(method = "cv",
number = 5,
verboseIter=TRUE,
summaryFunction = maeSummary
)
grid <- expand.grid(depth = c(5),
learning_rate = c(0.0005),
iterations = c(800),
l2_leaf_reg = c(1e-3),
rsm = c(0.95),
border_count = c(32)
)
traindata <- train[,2:(ncol(train)-1)]
target <- train[,logerror]
cb <- train(y=target,
x=traindata,
preProcess=NULL,
method=catboost.caret,
metric = "MAE",
maximize = FALSE,
tuneGrid = grid,
trControl = control
)
cb <- train(y=target,
x=traindata,
preProcess=NULL,
method=xgbLinear,
metric = "MAE",
maximize = FALSE,
tuneGrid = grid,
trControl = control
)
install.packages("xgboost")
library(xgboost)
cb <- train(y=target,
x=traindata,
preProcess=NULL,
method=xgbLinear,
metric = "MAE",
maximize = FALSE,
tuneGrid = grid,
trControl = control
)
library(xgboost)
cb <- train(y=target,
x=traindata,
preProcess=NULL,
method=xgbLinear,
metric = "MAE",
maximize = FALSE,
tuneGrid = grid,
trControl = control
)
getModelInfo()
install.packages("Metrics")
library(Metrics)
maeSummary <- function (train,
lev = NULL,
model = NULL) {
out <- mae(train$obs, train$pred)
names(out) <- "MAE"
out
}
control <- trainControl(method = "cv",
number = 5,
verboseIter=TRUE,
summaryFunction = maeSummary
)
grid <- expand.grid(depth = c(5),
learning_rate = c(0.0005),
iterations = c(800),
l2_leaf_reg = c(1e-3),
rsm = c(0.95),
border_count = c(32)
)
traindata <- train[,2:(ncol(train)-1)]
target <- train[,logerror]
cb <- train(y=target,
x=traindata,
preProcess=NULL,
method=xgbLinear,
metric = "MAE",
maximize = FALSE,
tuneGrid = grid,
trControl = control
)
cb <- train(y=target,
x=traindata,
preProcess=NULL,
method= "xgbLinear",
metric = "MAE",
maximize = FALSE,
tuneGrid = grid,
trControl = control
)
xgb_grid_1 = expand.grid(
nrounds = c(100),
eta = c(.1),
max_depth = c(10),
gamma = c(.8),
colsample_bytree = .5,
min_child_weight = 1
)
xgb_grid_1 = expand.grid(
nrounds = c(100),
eta = c(.1),
max_depth = c(10),
gamma = c(.8),
colsample_bytree = .5,
min_child_weight = 1
)
traindata <- train[,2:(ncol(train)-1)]
target <- train[,logerror]
cb <- train(y=target,
x=traindata,
preProcess=NULL,
method= "xgbLinear",
metric = "MAE",
maximize = FALSE,
tuneGrid = xgb_grid_1,
trControl = control
)
modelLookup("xgbLinear")
xgb_grid_1 = expand.grid(
nrounds = c(100),
eta = c(.5),
lambda = c(.5),
alpha = c(.5),
)
xgb_grid_1 = expand.grid(
nrounds = c(100),
eta = c(.5),
lambda = c(.5),
alpha = c(.5)
)
traindata <- train[,2:(ncol(train)-1)]
target <- train[,logerror]
cb <- train(y=target,
x=traindata,
preProcess=NULL,
method= "xgbLinear",
metric = "MAE",
maximize = FALSE,
tuneGrid = xgb_grid_1,
trControl = control
)
traindata <- as.matrix(train[,2:(ncol(train)-1)])
head(traindata)
target <- train[,logerror]
cb <- train(y=target,
x=traindata,
preProcess=NULL,
method= "xgbLinear",
metric = "MAE",
maximize = FALSE,
tuneGrid = xgb_grid_1,
trControl = control
)
traindata <- xgb.DMatrix(as.matrix((train[,2:(ncol(train)-1)])))
library(randomForest)
library(ggplot2)
library(ROCR)
library(xgboost)
library(dplyr)
require(caret)
library(pROC)
set.seed(181)
setwd('/Users/carmenlouise/documents/R/forest')
df <- read.csv('forest_data.csv',stringsAsFactors = FALSE)
df$Class <- as.factor(df$Class)
labels <- as.factor(df$Class)
df$Class <- factor(df$Class,labels = c("Failure", "Success"))
varnames <- colnames(df)
varnames <- setdiff(varnames,c('Class'))
rf.form <- as.formula(paste("Class~",paste(varnames,collapse="+")))
index <- sample(1:nrow(df),size = 0.6*nrow(df))
train <- df[index,]
test <- df[-index,]
test.labels <- labels[-index]
train.labels <- labels[index]
View(train)
train2 <- xgb.DMatrix(as.matrix(train))
train2 <- xgb.DMatrix((train))
typeof(train)
train2 <- xgb.DMatrix(as.data.frame(train))
as.data.frame(train)
train2 <- xgb.DMatrix(as.data.frame(train))
train2 <- xgb.DMatrix(as.matrix(train))
xgb_grid_1 = expand.grid(
nrounds = c(100,500,1000),
eta = c(.1,0.01, 0.001, 0.0001),
max_depth = c(2, 4, 6, 8, 10),
gamma = c(0,.5,1),
colsample_bytree = .5,
min_child_weight = 1
)
xgb_trcontrol_1 = trainControl(
method = "cv",
number = 5,
verboseIter = TRUE,
returnData = FALSE,
returnResamp = "all",
classProbs = TRUE,
summaryFunction = twoClassSummary,
allowParallel = TRUE
)
xgb_train_1 = train(
x = as.matrix(train %>%
select(-Class)),
y = as.factor(train$Class),
trControl = xgb_trcontrol_1,
tuneGrid = xgb_grid_1,
method = "xgbTree"
)
library(randomForest)
library(ggplot2)
library(ROCR)
library(xgboost)
library(dplyr)
require(caret)
library(pROC)
set.seed(181)
setwd('/Users/carmenlouise/documents/R/forest')
df <- read.csv('forest_data.csv',stringsAsFactors = FALSE)
View(df)
library(reshape2)
library(data.table)
library(ggplot2)
library(dplyr)
library(caret)
library(xgboost)
library(Metrics)
options(scipen = 999)
prop <- fread('/Users/carmenlouise/Documents/Zillow/properties_2016.csv')
t <- fread('/Users/carmenlouise/Documents/Zillow/train_2016_v2.csv')
sample <- fread("/Users/carmenlouise/Documents/Zillow/sample_submission.csv", header = TRUE)
View(prop)
library(reshape2)
library(data.table)
library(ggplot2)
library(dplyr)
library(caret)
library(xgboost)
library(Metrics)
options(scipen = 999)
prop <- fread('/Users/carmenlouise/Documents/Zillow/properties_2016.csv')
t <- fread('/Users/carmenlouise/Documents/Zillow/train_2016_v2.csv')
sample <- fread("/Users/carmenlouise/Documents/Zillow/sample_submission.csv", header = TRUE)
ctypes <- sapply(prop, class)
cidx <- which(ctypes %in% c("character", "factor"))
for(i in cidx){
prop[[i]] <- as.integer(factor(data[[i]]))
}
library(reshape2)
library(data.table)
library(ggplot2)
library(dplyr)
library(caret)
library(xgboost)
library(Metrics)
options(scipen = 999)
prop <- fread('/Users/carmenlouise/Documents/Zillow/properties_2016.csv')
t <- fread('/Users/carmenlouise/Documents/Zillow/train_2016_v2.csv')
sample <- fread("/Users/carmenlouise/Documents/Zillow/sample_submission.csv", header = TRUE)
ctypes <- sapply(prop, class)
cidx <- which(ctypes %in% c("character", "factor"))
for(i in cidx){
prop[[i]] <- as.integer(factor(prop[[i]]))
}
str(prop)
param <- list(  objective           = "reg:linear",
booster             = "gbtree",
eval_metric         = "mae",
eta                 = eta,
max_depth           = m_depth,
subsample           = 0.5,
colsample_bytree    = 0.5,
min_child_weight    = 4,
maximize            = FALSE
)
modelLookup("gbtree")
train <- merge(prop2, t, by="parcelid", all.y=TRUE)
train <- merge(prop, t, by="parcelid", all.y=TRUE)
rm(t)
rm(prop2)
rm(sample)
p[, logerror := 0]
prop[, logerror := 0]
gc()
rm(prop)
gc()
set.seed(181)
train %>% filter(logerror != 0) %>% qplot(train$logerror)
qplot(train$logerror)
maeSummary <- function (train,
lev = NULL,
model = NULL) {
out <- mae(train$obs, train$pred)
names(out) <- "MAE"
out
}
control <- trainControl(method = "cv",
number = 5,
verboseIter=TRUE,
summaryFunction = maeSummary
)
xgb_grid_1 = expand.grid(
nrounds = c(100),
eta = c(.5),
lambda = c(.5),
alpha = c(.5)
)
traindata <- xgb.DMatrix(as.matrix((train[,2:(ncol(train)-1)])))
target <- train[,logerror]
cb <- train(y=target,
x=traindata,
preProcess=NULL,
method= "xgbLinear",
metric = "MAE",
maximize = FALSE,
tuneGrid = xgb_grid_1,
trControl = control
)
library(reshape2)
library(data.table)
library(ggplot2)
library(dplyr)
library(caret)
library(xgboost)
library(Metrics)
options(scipen = 999)
prop <- fread('/Users/carmenlouise/Documents/Zillow/properties_2016.csv')
t <- fread('/Users/carmenlouise/Documents/Zillow/train_2016_v2.csv')
sample <- fread("/Users/carmenlouise/Documents/Zillow/sample_submission.csv", header = TRUE)
ctypes <- sapply(prop, class)
cidx <- which(ctypes %in% c("character", "factor"))
for(i in cidx){
prop[[i]] <- as.integer(factor(prop[[i]]))
}
print(cb)
cb_prediction <- predict(cb, prop)
cb_prediction
View(sample)
predictions <- round(as.vector(cb_prediction), 5)
result <- data.frame(cbind(prop$parcelid, predictions, predictions, predictions, predictions, predictions, predictions))
colnames(result) <- c("parcelid","201610","201611","201612","201710","201711","201712")
write.csv(result, file = "Carmen.csv", row.names = FALSE)
setwd("/Users/carmenlouise/Documents/Zillow")
write.csv(result, file = "Carmen.csv", row.names = FALSE)
getModelInfo(xgbLinear)
getModelInfo("xgbLinear")
modelLookup("xgbLinear")
prop2 <- filter(prop,logerror != 0)
View(sample)
library(reshape2)
library(data.table)
library(ggplot2)
library(dplyr)
library(caret)
library(xgboost)
library(Metrics)
options(scipen = 999)
setwd("/Users/carmenlouise/Documents/Zillow")
prop <- fread('/Users/carmenlouise/Documents/Zillow/properties_2016.csv')
t <- fread('/Users/carmenlouise/Documents/Zillow/train_2016_v2.csv')
sample <- fread("/Users/carmenlouise/Documents/Zillow/sample_submission.csv", header = TRUE)
ctypes <- sapply(prop, class)
cidx <- which(ctypes %in% c("character", "factor"))
for(i in cidx){
prop[[i]] <- as.integer(factor(prop[[i]]))
}
train <- merge(prop, t, by="parcelid", all.y=TRUE)
set.seed(181)
qplot(train$logerror)
# Enable caret to use MAE as eval metric
maeSummary <- function (train,
lev = NULL,
model = NULL) {
out <- mae(train$obs, train$pred)
names(out) <- "MAE"
out
}
modelLookup("xgbLinear")
control <- trainControl(method = "cv",
number = 5,
verboseIter=TRUE,
summaryFunction = maeSummary
)
xgb_grid_1 = expand.grid(
nrounds = c(1000),
eta = c(.01,.05,.08,.5),
lambda = c(0,.01,.05,.5,1),
alpha = c(0,.01,.05,.5,1)
)
traindata <- xgb.DMatrix(as.matrix((train[,2:(ncol(train)-1)])))
target <- train[,logerror]
cb <- train(y=target,
x=traindata,
preProcess=NULL,
method= "xgbLinear",
metric = "MAE",
maximize = FALSE,
tuneGrid = xgb_grid_1,
trControl = control
)
gc()
xgb_grid_1 = expand.grid(
nrounds = c(100),
eta = c(.01,.05,.08,.5),
lambda = c(0,.01,.05,.5,1),
alpha = c(0,.01,.05,.5,1)
)
xgb_grid_1 = expand.grid(
nrounds = c(100),
eta = c(.01,.05,.08,.5),
lambda = c(0,.01,.05,.5),
alpha = c(0,.01,.05,.5)
)
traindata <- xgb.DMatrix(as.matrix((train[,2:(ncol(train)-1)])))
target <- train[,logerror]
View(train)
traindata <- xgb.DMatrix(as.matrix((train[,1:(ncol(train)-1)])))
target <- train[,logerror]
cb <- train(y=target,
x=traindata,
preProcess=NULL,
method= "xgbLinear",
metric = "MAE",
maximize = FALSE,
tuneGrid = xgb_grid_1,
trControl = control
)
control <- trainControl(method = "cv",
number = 3,
verboseIter=TRUE,
summaryFunction = maeSummary
)
xgb_grid_1 = expand.grid(
nrounds = c(100),
eta = c(.01,.05,.08,.5),
lambda = c(0,.01,.05,.5),
alpha = c(0,.01,.05,.5)
)
traindata <- xgb.DMatrix(as.matrix((train[,1:(ncol(train)-1)])))
target <- train[,logerror]
cb <- train(y=target,
x=traindata,
preProcess=NULL,
method= "xgbLinear",
metric = "MAE",
maximize = FALSE,
tuneGrid = xgb_grid_1,
trControl = control
)
print(cb)
cb_prediction <- predict(cb, prop)
predictions <- round(as.vector(cb_prediction), 5)
result <- data.frame(cbind(prop$parcelid, predictions, predictions, predictions, predictions, predictions, predictions))
colnames(result) <- c("parcelid","201610","201611","201612","201710","201711","201712")
write.csv(result, file = "Carmen.csv", row.names = FALSE)
predictions
